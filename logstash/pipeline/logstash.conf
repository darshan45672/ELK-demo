input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["filebeat-logs"]
    codec => "json"
    consumer_threads => 1
    decorate_events => true
    group_id => "logstash-consumer-group"
  }
}

filter {
  # If the log is already parsed by Filebeat's NDJSON parser,
  # the fields will be at the root level
  # Only try to parse the message field if it's still a JSON string
  if [message] and [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }
    
    # If parsing succeeded, promote fields to root
    if [parsed] {
      ruby {
        code => "
          parsed = event.get('parsed')
          if parsed.is_a?(Hash)
            parsed.each { |k, v| event.set(k, v) unless k == 'message' }
          end
          event.remove('parsed')
        "
      }
    }
  }
  
  # Add processing metadata
  mutate {
    add_field => { 
      "pipeline_stage" => "logstash"
      "processed_at" => "%{@timestamp}"
    }
  }
  
  # Use the log's timestamp if available
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ" ]
      target => "@timestamp"
    }
  }
  
  # Remove duplicate or unnecessary fields
  mutate {
    remove_field => [ "agent.ephemeral_id", "agent.id", "ecs", "host.hostname" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "kafka-logstash-logs-%{+YYYY.MM.dd}"
  }
  
  # Debug output (optional - comment out in production)
  stdout {
    codec => rubydebug
  }
}
